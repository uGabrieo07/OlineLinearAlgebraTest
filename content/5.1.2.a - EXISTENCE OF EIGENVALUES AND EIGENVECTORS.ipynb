{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "f6214a90-550c-480a-8648-70330fec6423",
      "cell_type": "markdown",
      "source": "# Existence of Eigenvalues and Eigenvectors\n\n**Definition:** Given a linear transformation\n$\\mathbf{A}: \\mathcal{U} \\rightarrow \\mathcal{U}$ induced by a square\nmatrix $\\mathbf{A}$, we say that $\\mathbf{\\nu}$ is an eigenvector of\n$\\mathbf{A}$ associated with the eigenvalue $\\lambda$ if\n$\\mathbf{\\nu} \\neq 0$ and $$\\mathbf{A \\nu} = \\lambda \\mathbf{\\nu}$$ or,\nequivalently,\n$$\\mathbf{\\nu} \\in \\mathcal{N}(\\mathbf{A} - \\lambda \\mathbf{I})$$ i.e.,\n$\\nu$ belongs to the kernel, or null space, of the transformation\nrepresented by $\\mathbf{A} - \\lambda \\mathbf{I}$.\n\nAlthough we have avoided to include vector spaces defined over the field\nof complex numbers, eigenvalues and eigenvectors will force us to\nconsider eventual exceptions to this rule in the ensuing discussion.\n\nLet $\\mathbf{A}$ be a linear operator on $\\mathcal{U}$, a vector space\ndefined over the field of real numbers. Now let\n$\\mathcal{U}_{\\mathbb{C}} = \\mathcal{U} \\times \\mathcal{U}$ be a vector\nspace defined as\n$$\\mathcal{U}_{\\mathbb{C}} = \\{ \\nu_{\\mathbb{R}} + i \\nu_{\\mathbb{I}} : \\nu_{\\mathbb{R}}, \\nu_{\\mathbb{I}} \\in \\mathcal{U} \\}$$\nwhere $i = \\sqrt{-1}$. It is straightforward to show that $\\mathbf{A}$\nis a linear operator on $\\mathcal{U}_{\\mathbb{C}}$ as well.\n\nWe maintain that a linear operator $\\mathbf{A}$ on $\\mathcal{U}$\npreserves some directions of $\\mathcal{U}_{\\mathbb{C}}$. If a direction\nis invariant under the transformation, then we say that a vector lying\nin this direction is an eigenvector of the transformation $A$.\n\nBefore we proceed, we shall prove that\n$\\mathbf{A \\nu} = \\lambda \\mathbf{\\nu}$ is valid for a linear operator\n$\\mathbf{A}$ on $\\mathcal{U}$ and some pair\n$(\\lambda \\in \\mathbb{C}, \\nu \\in \\mathcal{U}_{\\mathbb{C}})$.\n\nLet $\\mathbf{A}$ be a linear operator on a finite-dimensional vector\nspace $\\mathcal{U}$ defined over the field of real numbers. There exists\na vector $\\nu \\in \\mathcal{U}_{\\mathbb{C}}$ and a scalar\n$\\lambda \\in \\mathbb{C}$ such that\n$\\mathbf{A\\nu} = \\lambda \\mathbf{\\nu}$.\n\nLet $n$ be the dimension of $\\mathcal{U}$. Therefore there exists a set\nof scalars $$\\alpha_0, \\alpha_1, \\dots, \\alpha_n$$, not all of them\nequal to zero, such that\n$$\\alpha_0 \\mathcal{U} + \\alpha_1 \\mathbf{A}\\mathcal{U} + \\cdots + \\alpha_n \\mathbf{A}^n \\mathcal{U} = 0$$\nfor some $\\nu \\neq 0, \\nu \\in \\mathcal{U}$. This is true because we\ncannot have $n+1$ linearly independent vectors in an $n$-dimensional\nvector space. We can rewrite the equation above as\n$$0 = (\\alpha_0 + \\alpha_1 \\mathbf{A} + \\cdots + \\alpha_n \\mathbf{A}^n)\\nu = c(\\mathbf{A} - \\beta_1 I)(\\mathbf{A} - \\beta_2 I)\\cdots(\\mathbf{A} - \\beta_n I)\\nu, \\quad c \\neq 0, \\beta_i \\in \\mathbb{C}$$\n\nwhich means that there is at least one pair\n$\\left(\\lambda = \\beta_i \\in \\mathbb{C}, \\mathbf{\\nu} \\in \\mathcal{U}_{\\mathbb{C}}\\right)$\nsuch that $(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{\\nu} = 0$.\n\nThe result just presented in the form of a theorem indicates that every\nlinear operator in $\\mathcal{U}$ has at least one pair of eigenvalue in\n$\\mathbb{C}$ and eigenvector in $\\mathcal{U}_{\\mathbb{C}}$. In fact,\nthere can be more than one pair of eigenvalues and corresponding\neigenvectors, and we can write $\\mathbf{A \\nu} = \\lambda \\mathbf{\\nu}$ in matrix form for $r$\neigenvalues and eigenvectors as follows:\n$$\\mathbf{A\\nu} = \\mathbf{V \\Lambda}$$ where\n$$\\mathbf{V} = [\\mathbf{\\nu_1 \\nu_2 \\dots \\nu_r}]$$ is a matrix whose\ncolumns are the eigenvectors of $\\mathbf{A}$, and $$\\mathbf{\\Lambda} = \n\\begin{bmatrix}\n    \\lambda_1 & 0 & \\cdots & 0 \\\\\n    0 & \\lambda_2 & 0 & \\cdots \\\\\n    \\vdots & 0 & \\ddots & 0 \\\\\n    0 & \\cdots & 0 & \\lambda_r\n\\end{bmatrix}$$ is a diagonal matrix with the associated eigenvalues.\n\nFrom the definitions above, we can say that an eigenvector of a linear\noperator $\\mathbf{A}$ spans an invariant subspace under $\\mathbf{A}$\nwith dimension equal to one. A single eigenvalue can sometimes be\nassociated with multiple linearly independent eigenvectors. In this\ncase, we say that this eigenvalue has geometrical multiplicity equal to\nthe number of one-dimensional invariant subspaces spanned by its\nassociated eigenvectors. We will see more of eigenvalue multiplicity\ntowards the end of the present chapter.\n\nWith the aid of a toy example, let us investigate the eigenvalues and\neigenvectors of a simple linear operator.\n\n**Ex:** Let $\\mathbf{A}$ be a linear operator in $\\mathbb{R}^2$, induced\nby matrix $$\\mathbf{A} = \\begin{bmatrix}\n    1 & 1 \\\\\n    2 & 0\n\\end{bmatrix}$$ By inspection, We will see iterative methods that are\ncapable of revealing eigenvalues and eigenvectors further in this\nChapter and the book, but for the moment we may defer involved\ncalculations and concentrate on the definitions. we can realize that\nvectors $\\mathbf{\\nu_1} = [1 \\ \\ 1]^{\\top}$ and\n$\\mathbf{\\nu_2} = [1 \\ \\ -2]^{\\top}$ both individually span subspaces of\n$\\mathbb{R}^2$ with dimension equal to one which are invariant under\n$\\mathbf{A}$. Therefore $\\nu_1$ and $\\nu_2$ are eigenvectors of $A$.\nNaturally, any vector $\\alpha \\nu_i$, where $\\alpha \\in \\mathbb{R}$, is\nalso an eigenvector.\n\n$\\alpha \\in \\mathbb{R}$ and $i = 1, 2$, is also an eigenvector of\n$\\mathbf{A}$, because scaling does not change direction. For instance,\nwe call this uniform scaling operation a homothety, or homogeneous\ndilation, with homothetic center at the origin. One particular homothety\nthat comes handy is the one which renders the eigenvector with unitary\nEuclidean norm:\n$\\mathbf{\\bar{\\nu}_j} = \\mathbf{\\nu_j} / \\|\\mathbf{\\nu_j}\\|_2$. The\nscalars which settle the equation $\\mathbf{A\\nu} = \\lambda \\mathbf{\\nu}$\nare the associated eigenvalues: $\\lambda_1 = 2$ and $\\lambda_2 = -1$,\nrespectively. We may construct matrices $\\mathbf{V}$ and\n$\\mathbf{\\Lambda}$ as $$\\mathbf{V} = \\begin{bmatrix}\n1 & 1 \\\\\n1 & -2\n\\end{bmatrix}, \\quad \n\\mathbf{\\Lambda} = \\begin{bmatrix}\n2 & 0 \\\\\n0 & -1\n\\end{bmatrix}$$ such that $$\\mathbf{A V} = \\mathbf{V \\Lambda}$$\n\nWe can also verify that similar matrices share the same set of\neigenvalues, according to the theorem below.\n\nLet $\\mathbf{A}$ and $\\mathbf{B}$ be similar matrices, i.e., there\nexists an invertible linear transformation $\\mathbf{P}$ such that\n$\\mathbf{A} = \\mathbf{P^{-1}BP}$. If $\\lambda$ is an eigenvalue of\n$\\mathbf{A}$ associated with the eigenvector $\\mathbf{\\nu}$, then\n$\\lambda$ is also an eigenvalue of $\\mathbf{B}$, but associated with the\neigenvector $\\mathbf{x} = \\mathbf{P\\nu}$.\n\nIf $\\lambda$ is an eigenvalue of $\\mathbf{A}$, then\n$\\mathbf{A\\nu} = \\lambda \\mathbf{\\nu}$. Furthermore, if $\\mathbf{A}$ and\n$\\mathbf{B}$ are similar, then $\\mathbf{PA} = \\mathbf{BP}$. Therefore,\n$\\mathbf{PA\\nu} = \\mathbf{BP\\nu}$. Now let $x = P\\nu$,\n\n$$\\mathbf{BP\\nu = Bx = PA\\nu =} \\lambda \\mathbf{P \\nu} = \\lambda \\mathbf{x} \\implies \\mathbf{B}x = \\lambda \\mathbf{x}$$\n\nIn the previous example, we were able to identify two pairs of\neigenvalue and eigenvector. Indeed, the next theorem and corollary show\nthat the number of eigenvalues, $r$, cannot exceed $n$, the dimension of\n$\\mathcal{U}$.\n\n\n",
      "metadata": {}
    },
    {
      "id": "1b67f677-28f0-4b2e-8f4c-653bb93fff8e",
      "cell_type": "markdown",
      "source": "### Using the computer to calculate the Eigenvalues and Eigenvectors of a Matrix A\nThis interactive example demonstrates how to compute the eigenvalues and eigenvectors of a **2x2 matrix**, and visualizes the corresponding eigenvectors in a **2D space**. You can input the elements of the matrix using four **input fields**, which represent the values in the matrix. Once the values are provided and the ***\"Calculate\"*** button is clicked, the program computes the eigenvalues and eigenvectors of the matrix using NumPy's eig function. The eigenvalues represent the scalar values that define the magnitude of the stretching or shrinking of the eigenvectors during a linear transformation. The eigenvectors define the direction of this transformation.\n\nRemember that the eigenvalues and it's associated eigenvectors, must obey\n$$\\mathbf{A \\nu} = \\lambda \\mathbf{\\nu}$$  \nand  \n$$(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{\\nu} = 0$$\n\nAfter calculating the eigenvalues and eigenvectors, the system prints them out. For each eigenvalue, it shows the corresponding eigenvector. Additionally, the visualization of the eigenvectors is plotted in a 2D coordinate system. The red and blue arrows represent the eigenvectors, helping to illustrate how the matrix affects vectors during the transformation.\n\nEigenvectors are significant in many fields of study, such as physics, data analysis, and control systems, as they simplify complex transformations by focusing on **directions that remain consistent** (invariant subspaces) under transformation (even if the scale changes). \n",
      "metadata": {}
    },
    {
      "id": "67fa1246-012f-4b38-bec4-98fcd2e95fb0",
      "cell_type": "code",
      "source": "%pip install -q ipywidgets==8.0.7 ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "id": "5082f4fb-af9e-4602-afee-856a0ce17e16",
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output, Markdown",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "id": "ee240c12-e3ea-473a-b2cd-7747067521dc",
      "cell_type": "code",
      "source": "\n\n# Function to compute eigenvalues and eigenvectors\ndef compute_eigen(matrix):\n    \"\"\"Computes the eigenvalues and eigenvectors of the matrix.\"\"\"\n    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n    return eigenvalues, eigenvectors\n\n# Function to plot eigenvectors in 2D\ndef plot_eigenvectors(matrix, eigenvalues, eigenvectors):\n    \"\"\"Plots the eigenvectors in a 2D space for a 2x2 matrix.\"\"\"\n    if matrix.shape[0] != 2:\n        print(\"Plotting is only available for 2x2 matrices.\")\n        return\n\n    origin = np.zeros((2, 2))  # Two vectors originate from (0,0)\n\n    # Eigenvectors are columns of the eigenvectors matrix\n    plt.quiver([origin[0, 0], origin[0, 1]], [origin[1, 0], origin[1, 1]], eigenvectors[0,:], eigenvectors[1,:], color=['r','b'], scale=1, scale_units='xy', angles='xy')\n\n    # Set the plot limits\n    plt.xlim(-3, 3)\n    plt.ylim(-3, 3)\n    plt.axhline(0, color='black', linewidth=0.5)\n    plt.axvline(0, color='black', linewidth=0.5)\n    \n    plt.title(f\"Eigenvectors and Eigenvalues\\nEigenvalues: {eigenvalues}\")\n    plt.grid(True)\n    plt.show()\n\nnp.set_printoptions(precision=4, suppress=True)\n\n# Function that updates the plot based on widget inputs\ndef update_matrix(a11, a12, a21, a22):\n    matrix = np.array([[a11, a12], [a21, a22]])\n    \n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = compute_eigen(matrix)\n    \n    # Print eigenvalues and eigenvectors\n    with output:\n        output.clear_output()\n        display(Markdown(\"#### Matrix A:\"))\n        display(matrix)\n        display(Markdown(\"#### Eigenvalues:\"))\n        print(eigenvalues)\n        display(Markdown(f\"#### Eigenvector associated with {eigenvalues[0]:.4f}:\"))\n        print(f\"({eigenvectors[0][0]:.4f}, {eigenvectors[1][0]:.4f})\")\n        display(Markdown(f\"#### Eigenvector associated with {eigenvalues[1]:.4f}:\"))\n        print(f\"({eigenvectors[0][1]:.4f}, {eigenvectors[1][1]:.4f})\")\n        \n    \n        # Plot the eigenvectors\n        plot_eigenvectors(matrix, eigenvalues, eigenvectors)\n\noutput = widgets.Output()\n\n# Create widgets for inputting the elements of a 2x2 matrix\na11 = widgets.IntText(value=1, description='', step=1, layout=widgets.Layout(width='50px'))\na12 = widgets.IntText(value=2, description='', step=1, layout=widgets.Layout(width='50px'))\na21 = widgets.IntText(value=3, description='', step=1, layout=widgets.Layout(width='50px'))\na22 = widgets.IntText(value=4, description='', step=1, layout=widgets.Layout(width='50px'))\n\n# Arrange the matrix inputs in a grid\nmatrix_inputs = widgets.GridBox([a11, a12, a21, a22], layout=widgets.Layout(grid_template_columns=\"60px 60px\"))\nbutton = widgets.Button(description=\"Calculate\")\nbutton.on_click(lambda b: update_matrix(a11.value, a12.value, a21.value, a22.value))\n\n# Display the matrix inputs and output\ndisplay(Markdown(\"#### Matrix A inputs:\"))\ndisplay(matrix_inputs, button, output)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "#### Matrix A inputs:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "GridBox(children=(IntText(value=1, layout=Layout(width='50px')), IntText(value=2, layout=Layout(width='50px'))…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1f5de15e111408e9a9b724639b73c05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Button(description='Calculate', style=ButtonStyle())",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9fe86ca03554684bd5b993d451cf38b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Output()",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fb666e6c847456ebae6ff8083e21603"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 42
    },
    {
      "id": "3611c396-5a7c-462b-8cc6-7da51544ad50",
      "cell_type": "markdown",
      "source": "Lets remake the previous example, but now for a **3x3 matrix**. The result of our calculations will be plotted in a **3D space**. ",
      "metadata": {}
    },
    {
      "id": "ad475087-a197-4151-b5aa-57b25a88ba03",
      "cell_type": "code",
      "source": "#importing the library that'll allow us to plot a 3D space\nfrom mpl_toolkits.mplot3d import Axes3D",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 43
    },
    {
      "id": "542bc117-b874-4668-8077-8ebe2a072746",
      "cell_type": "code",
      "source": "np.set_printoptions(precision=4, suppress=True)\n\n# Function to compute eigenvalues and eigenvectors\ndef compute_eigen(matrix):\n    \"\"\"Computes the eigenvalues and eigenvectors of the matrix.\"\"\"\n    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n    return eigenvalues, eigenvectors\n\n# Function to plot eigenvectors in 3D\ndef plot_eigenvectors(matrix, eigenvalues, eigenvectors):\n    \"\"\"Plots the eigenvectors in a 3D space for a 3x3 matrix.\"\"\"\n    if matrix.shape[0] != 3:\n        print(\"Plotting is only available for 3x3 matrices.\")\n        return\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    origin = np.zeros((3, 3))  # Vectors originating from (0,0,0)\n\n    # Eigenvectors are columns of the eigenvectors matrix\n    ax.quiver(*origin, eigenvectors[0,:], eigenvectors[1,:], eigenvectors[2,:], color=['r','b','g'])\n\n    # Set plot limits\n    ax.set_xlim([-3, 3])\n    ax.set_ylim([-3, 3])\n    ax.set_zlim([-3, 3])\n\n    ax.set_xlabel('X axis')\n    ax.set_ylabel('Y axis')\n    ax.set_zlabel('Z axis')\n\n    plt.title(f\"Eigenvectors and Eigenvalues\\nEigenvalues: {eigenvalues}\")\n    plt.grid(True)\n    plt.show()\n\n# Function that updates the plot based on widget inputs\ndef update_matrix(a11, a12, a13, a21, a22, a23, a31, a32, a33):\n    matrix = np.array([[a11, a12, a13], [a21, a22, a23], [a31, a32, a33]])\n    \n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = compute_eigen(matrix)\n    \n    # Print eigenvalues and eigenvectors\n    with output:\n        output.clear_output()\n        display(Markdown(\"#### Matrix A:\"))\n        display(matrix)\n        display(Markdown(\"#### Eigenvalues:\"))\n        print(eigenvalues)\n        display(Markdown(f\"#### Eigenvector associated with {eigenvalues[0]:.4f}:\"))\n        print(f\"({eigenvectors[0][0]:.4f}, {eigenvectors[1][0]:.4f}, {eigenvectors[2][0]:.4f})\")\n        display(Markdown(f\"#### Eigenvector associated with {eigenvalues[1]:.4f}:\"))\n        print(f\"({eigenvectors[0][1]:.4f}, {eigenvectors[1][1]:.4f}, {eigenvectors[2][1]:.4f})\")\n        display(Markdown(f\"#### Eigenvector associated with {eigenvalues[2]:.4f}:\"))\n        print(f\"({eigenvectors[0][2]:.4f}, {eigenvectors[1][2]:.4f}, {eigenvectors[2][2]:.4f})\")\n    \n        # Plot the eigenvectors\n        plot_eigenvectors(matrix, eigenvalues, eigenvectors)\n\noutput = widgets.Output()\n\n# Create IntText widgets for inputting the elements of a 3x3 matrix\na11 = widgets.IntText(value=1, description='', step=1, layout=widgets.Layout(width='50px'))\na12 = widgets.IntText(value=2, description='', step=1, layout=widgets.Layout(width='50px'))\na13 = widgets.IntText(value=3, description='', step=1, layout=widgets.Layout(width='50px'))\na21 = widgets.IntText(value=4, description='', step=1, layout=widgets.Layout(width='50px'))\na22 = widgets.IntText(value=5, description='', step=1, layout=widgets.Layout(width='50px'))\na23 = widgets.IntText(value=6, description='', step=1, layout=widgets.Layout(width='50px'))\na31 = widgets.IntText(value=7, description='', step=1, layout=widgets.Layout(width='50px'))\na32 = widgets.IntText(value=8, description='', step=1, layout=widgets.Layout(width='50px'))\na33 = widgets.IntText(value=9, description='', step=1, layout=widgets.Layout(width='50px'))\n\n# Arrange the matrix inputs in a grid\nmatrix_inputs = widgets.GridBox([a11, a12, a13, a21, a22, a23, a31, a32, a33], \n                                layout=widgets.Layout(grid_template_columns=\"60px 60px 60px\"))\nbutton = widgets.Button(description=\"Calculate\")\nbutton.on_click(lambda b: update_matrix(a11.value, a12.value, a13.value, \n                                        a21.value, a22.value, a23.value, \n                                        a31.value, a32.value, a33.value))\n\n# Display the matrix inputs and output\ndisplay(Markdown(\"#### Matrix A inputs:\"))\ndisplay(matrix_inputs, button, output)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "#### Matrix A inputs:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "GridBox(children=(IntText(value=1, layout=Layout(width='50px')), IntText(value=2, layout=Layout(width='50px'))…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4601a65f042244b38e267ec34a3cdf63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Button(description='Calculate', style=ButtonStyle())",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f34d32fccb614cc3a548fe4c4c4fa627"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Output()",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0a18d9a10e74af48e8d5fa38364d0e3"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 47
    },
    {
      "id": "899d096f-a42c-44ba-998c-ceb25478ea56",
      "cell_type": "markdown",
      "source": "#### Questions you should make:\nWhat can you infer about the dimension of the matrices and it's eigenvalues?  \nWhats happens if we try to use a non-square matrix? The eigenvalues and eigenvectors will still exist?\n",
      "metadata": {}
    }
  ]
}
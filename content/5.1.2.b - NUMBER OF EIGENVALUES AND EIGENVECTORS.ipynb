{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "0a9df347-6584-464b-bbbf-656e1f795d44",
      "cell_type": "markdown",
      "source": "# Number of Eigenvalues and Eigenvectors\n\nLet **A** be a linear operator on a finite-dimensional vector\n$\\mathcal{U}$ defined over the field of real numbers. Nonzero\neigenvectors of **A** corresponding to distinct eigenvalues are linearly\nindependent.\n\nLet *m* be the number of distinct eigenvalues, and ${\\alpha i}$, i = 1,\n‚Ä¶, *m* be a set of complex numbers such that\n$$\\alpha_1 \\mathbf{v_1} + \\alpha_2 \\mathbf{v_2} + \\dots + \\alpha_\\textit{m} \\mathbf{v_\\textit{m}} = 0$$\nwhere $\\mathbf{v_i}$ is an eigenvector of $\\mathbf{A}$ corresponding to\nthe eigenvalue $\\lambda_i$. If we premultiply both sides of the equation\nabove by\n$(\\mathbf{A} - \\lambda_2 \\mathbf{I}) (\\mathbf{A} - \\lambda_3 \\mathbf{I}) \\dots (\\mathbf{A} - \\lambda_m \\mathbf{I})$,\nwe have\n$$\\alpha_1(\\lambda_1 - \\lambda_2)(\\lambda_1 - \\lambda_3)\\dots(\\lambda_1-\\lambda_m)\\mathbf{v}_1 = 0$$\nwhich implies $\\alpha_1 = 0$, because $\\lambda_i \\ne \\lambda_j$ for\n$i \\ne j$. If we repeat the procedure for\n$(\\mathbf{A} - \\lambda_1 \\mathbf{I})(\\mathbf{A} - \\lambda_3 \\mathbf{I}) \\dots (\\mathbf{A} - \\lambda_m \\mathbf{I})$,\nwe get $\\alpha_2 = 0$. After we repeat the procedure for all\n$\\mathbf{v_i}$, $i = 1, \\dots, m$, we conclude that\n$\\alpha_1 = \\alpha_2 = \\dots = \\alpha_m = 0$, therefore the vectors\n$\\mathbf{v_1}, \\dots, \\mathbf{v_m}$ are linearly independent.\n\nThe number of eigenvalues of **A** cannot exceed *n*, the dimension of\nthe vector space $\\mathcal{U}$.\n\n**Evidence:** This corollary follows immediately from the theorem, for\nwe cannot have more than *n* linearly independent vectors in a vector\nspace whose dimension is *n*.\n\nWe have just seen that each linear operator has at least one pair of\neigenvalue and eigenvector, that eigenvectors associated with distinct\neigenvalues are linearly independent, and that the maximum number of\ndistinct eigenvalues is the dimension of the vector space, *n*.\nEquivalently, we can also say that the number of linearly independent\neigenvectors cannot exceed *n*, the dimension of the vector space.\nFurthermore, we may also find cases in which a single eigenvalue is\nassociated with multiple linearly independent eigenvectors, which\ncharacterizes an eigenvalue of geometrical multiplicity greater than\none. However, can we ascertain that n linearly independent eigenvectors\ndo exist? In other words, do the eigenvectors of a linear operator span\nthe whole vector space? In some cases, yes, but unfortunately, the\nanswer is no for the general case: the eigenvectors of a linear operator\nA in $\\mathcal{U}$ do not necessarily span $\\mathcal{U}$ .\n\nIf **A** has *n* distinct and real eigenvalues, then there are *n*\nlinearly independent corresponding eigenvectors, which span\n$\\mathbb{R}^n$. This is equivalent to saying that if **A** has *n*\ndistinct and real eigenvalues, then the corresponding eigenvectors form\na basis of $\\mathbb{R}^n$.\n\nLet us investigate a toy example.\n\n$$\\mathbf{A} = \n            \\begin{bmatrix}\n            1 & 1 \\\\\n            2 & 0\n            \\end{bmatrix}$$ By inspection, e can identify\n$\\mathbf{v} = [1\\;0]^T$ as eigenvector of A associated with the\neigenvalue $\\lambda = 1$. We shall anticipate that any attempt to find\nanother eigenvector of **A** linearly independent with respect to **v**\nwill be frustrating. We will soon learn that the eigenvalues of a\ntriangular matrix are revealed on its main diagonal. Therefore, the\neigenvalues in this example are both equal to one,\n$\\lambda_1 = \\lambda_2 = 1$, and the previous Corollary does not apply.\nThe eigenvalue $\\lambda = 1$ has geometrical multiplicity equal to 1.\n\n",
      "metadata": {}
    },
    {
      "id": "63f60910-065d-470f-9453-d9f94679bea8",
      "cell_type": "markdown",
      "source": "### Algebraic and Geometric Multiplicity of Eigenvalues\n\nWhen studying the eigenvalues and eigenvectors of a linear operator A, it's essential to understand two important concepts: the algebraic multiplicity and the geometric multiplicity of an eigenvalue.\n\n**Algebraic Multiplicity**  \nThe algebraic multiplicity of an eigenvalue refers to how many times a specific eigenvalue appears as a root of the characteristic polynomial of the linear operator A. For example, if we have a polynomial of degree 2, and ùúÜùëñ is a root of that polynomial, we say that ùúÜùëñ is an eigenvalue of A with algebraic multiplicity 2. This means ùúÜùëñ appears twice in the factorization of the characteristic polynomial.\n\n**Geometric Multiplicity**  \nOn the other hand, the geometric multiplicity of an eigenvalue describes the number of linearly independent eigenvectors associated with that eigenvalue. Specifically, if an eigenvalue ùúÜùëñ has a geometric multiplicity greater than 1, it means there are at least two linearly independent eigenvectors associated with ùúÜùëñ. In simpler terms, the geometric multiplicity is the dimension of the subspace (or eigenspace) associated with that eigenvalue.\n\n**In summary:**  \nAlgebraic multiplicity tells us how many times an eigenvalue appears as a root of the characteristic polynomial.  \nGeometric multiplicity tells us how many linearly independent eigenvectors exist for that eigenvalue.\n\nLet‚Äôs explore these ideas further with an interactive example. In this example, you will be able to input a matrix \nùê¥\nA of any size and see the algebraic and geometric multiplicities of its eigenvalues, as well as the number of distinct eigenvalues. After that, the eigenvalues will be plotted on the Complex Plane. Try changing the Matrix A elements and then check the results. This will help solidify your understanding of these important concepts in linear algebra.",
      "metadata": {}
    },
    {
      "id": "00421bbf-631c-4aa6-a498-ecfb68842b88",
      "cell_type": "code",
      "source": "%pip install -q ipywidgets==8.0.7 ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 70
    },
    {
      "id": "4627e79d-7b42-4a5d-aed2-e63508613bd0",
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom IPython.display import display, Markdown\nfrom numpy.linalg import matrix_rank",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 71
    },
    {
      "id": "699576b6-aed7-47bb-91f0-addbdc3e5d09",
      "cell_type": "code",
      "source": "# Function to compute eigenvalues and eigenvectors\ndef compute_eigen(matrix):\n    \"\"\"Computes the eigenvalues and eigenvectors of the matrix.\"\"\"\n    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n    return eigenvalues, eigenvectors\n\n# Function to calculate geometric multiplicity\ndef calculate_geometric_multiplicity(matrix, eigenvalue):\n    lambda_identity = eigenvalue * np.eye(matrix.shape[0])\n    eigenspace_matrix = matrix - lambda_identity\n    return matrix.shape[0] - matrix_rank(eigenspace_matrix)\n\n# Function to display eigenvalues, algebraic and geometric multiplicities\ndef display_multiplicities(matrix, eigenvalues):\n    unique_eigenvalues, counts = np.unique(np.round(eigenvalues, decimals=5), return_counts=True)\n    for i, val in enumerate(unique_eigenvalues):\n        alg_multiplicity = counts[i]\n        geo_multiplicity = calculate_geometric_multiplicity(matrix, val)\n        print(f\"Eigenvalue {val:.4f} - Algebraic Multiplicity: {alg_multiplicity}, Geometric Multiplicity: {geo_multiplicity}\")\n\n# Function to plot eigenvalues on the complex plane\ndef plot_eigenvalues(eigenvalues):\n    plt.figure(figsize=(6, 6))\n    plt.axhline(0, color='black', linewidth=0.5)\n    plt.axvline(0, color='black', linewidth=0.5)\n    plt.scatter(eigenvalues.real, eigenvalues.imag, color=\"red\")\n    plt.xlabel(\"Real Part\")\n    plt.ylabel(\"Imaginary Part\")\n    plt.title(\"Eigenvalues on the Complex Plane\")\n    plt.grid(True)\n    plt.show()\n\n# Function to update the output based on widget inputs\ndef update_matrix(matrix_values, matrix_size):\n    matrix = np.array(matrix_values).reshape(matrix_size, matrix_size)\n    \n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = compute_eigen(matrix)\n    \n    # Display matrix, eigenvalues, and eigenvectors\n    with output:\n        output.clear_output()\n        display(Markdown(f\"#### Matrix A ({matrix_size}x{matrix_size}):\"))\n        display(matrix)\n        \n        # Display eigenvalues and associated multiplicities\n        display(Markdown(\"#### Eigenvalues, Algebraic and Geometric Multiplicities:\"))\n        display_multiplicities(matrix, eigenvalues)\n        \n        # Plot the eigenvalues on the complex plane\n        plot_eigenvalues(eigenvalues)\n\noutput = widgets.Output()\n\n# Function to create matrix input widgets based on selected size\ndef create_matrix_inputs(matrix_size):\n    matrix_inputs = []\n    for i in range(matrix_size * matrix_size):\n        matrix_inputs.append(widgets.FloatText(value=1, layout=widgets.Layout(width='50px')))\n    return matrix_inputs\n\n# Function to update matrix size and inputs when \"Set Dimension\" is clicked\ndef update_matrix_size(b):\n    np.set_printoptions(precision=4, suppress=True)\n    matrix_size = int(dimension_dropdown.value)\n    # Clear previous output and input widgets\n    output.clear_output()\n    # Create matrix input widgets\n    matrix_inputs = create_matrix_inputs(matrix_size)\n    # Arrange the matrix inputs in a grid\n    grid_layout = widgets.GridBox(matrix_inputs, layout=widgets.Layout(grid_template_columns=f\"{' '.join(['50px'] * matrix_size)}\"))\n    # Display matrix inputs in the output area\n    with output:\n        display(Markdown(f\"#### Enter Elements for {matrix_size}x{matrix_size} Matrix:\"))\n        display(grid_layout)\n    # Update the \"Calculate\" button to use the latest matrix inputs\n    calculate_button.on_click(lambda b: update_matrix([widget.value for widget in matrix_inputs], matrix_size))\n\n# Dropdown for selecting matrix size\ndimension_dropdown = widgets.Dropdown(\n    options=['2', '3', '4', '5'],\n    value='2',\n    description='Matrix Size:',\n    layout=widgets.Layout(width='130px')\n)\n\n# Button to set the dimension and display matrix inputs\nset_dimension_button = widgets.Button(description=\"Set Dimension\")\nset_dimension_button.on_click(update_matrix_size)\n\n# Calculate button for performing matrix calculations\ncalculate_button = widgets.Button(description=\"Calculate\")\n\n# Display dropdown, set dimension button, calculate button, and output area\ndisplay(Markdown(\"#### Select Matrix Size:\"))\ndisplay(dimension_dropdown, set_dimension_button, calculate_button, output)\n\n# PQ N√ÉO TA APARECENDO O BOTAO DE CALCULAAAAAAAAAAAAAAAAARRRRR??????????????????\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "#### Select Matrix Size:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Dropdown(description='Matrix Size:', layout=Layout(width='130px'), options=('2', '3', '4', '5'), value='2')",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1540b973c5d642a485b1e44261bdc7e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Button(description='Set Dimension', style=ButtonStyle())",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8283acd5b12a4784a53f6fa3d3db9807"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Button(description='Calculate', style=ButtonStyle())",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44b84f48da83421095bd9cec640cefa2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Output()",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4ea950651f24f97a62ba83e6f9fd984"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 72
    },
    {
      "id": "8cde5817-1f2f-4952-8807-6dba171d35fb",
      "cell_type": "markdown",
      "source": "# Diagonalizing Matrices\n\nIf possible, it is often useful to work with diagonal matrices which are\nsimilar to the square, but otherwise generic matrix $\\mathbf{A}$. This\nrequires the existence of a similarity transformation which yields a\nmatrix $\\mathbf{\\Lambda}$ diagonal and similar to $\\mathbf{A}$. Given\n$\\mathbf{A}$, a linear operator in some vector space $\\mathcal{V}$ with\ndimension $n$, if its eigenvectors form a basis of $\\mathcal{V}$, then\nthe matrix $\\mathbf{V}$ constructed with the $n$ linearly independent\neigenvectors of $\\mathbf{A}$ is invertible. Therefore $\\mathbf{A}$ is\ndiagonalizable, because\n\n$$\\text{If } \\exists \\, \\mathbf{V}^{-1}, \\text{ then } \\mathbf{A} \\mathbf{V} = \\mathbf{V} \\mathbf{\\Lambda} \\iff \\mathbf{V}^{-1} \\mathbf{A} \\mathbf{V} = \\mathbf{\\Lambda}$$\n\nFrom the Corollary ?? given above, we can state the following theorem:\n\nLet a linear operator in $\\mathbb{R}^n$ be induced by matrix\n$\\mathbf{A}$ which has $n$ distinct and real eigenvalues. Matrices\n$\\mathbf{A}$ and $\\mathbf{\\Lambda}$, a diagonal matrix whose elements\nare the eigenvalues of $\\mathbf{A}$, are similar according to the\nDefinition ??.\n\nThe $n$ eigenvalues are real and distinct, therefore according to\nCorollary ?? the corresponding $n$ eigenvectors are linearly\nindependent. We can construct a matrix with the eigenvectors as\n\n$$\\mathbf{V} = \\begin{bmatrix} \\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_n \\end{bmatrix}$$\n\nwhich is invertible. Therefore\n\n$$\\mathbf{A} \\mathbf{V} = \\mathbf{V} \\mathbf{\\Lambda} \\iff \\mathbf{V}^{-1} \\mathbf{A} \\mathbf{V} = \\mathbf{\\Lambda}$$\n\nwhich concludes the proof.\n\nThe similarity between the linear operator $\\mathbf{A}$ and the diagonal\nmatrix $\\mathbf{\\Lambda}$ allows us to formulate the following remarks:\nLinear operators whose eigenvalues are distinct and real are\n**diagonalizable**, and the diagonalizing procedure is a similarity\ntransformation using the matrix formed by the eigenvectors. If the\neigenvectors of a linear operator A in U form a basis of U, then in such\nbasis the operator A is induced,or represented, by the diagonal matrix\n$\\Lambda$, the matrix of the eigenvalues of A. Although the requirement\nof *n* distinct and real eigenvalues can be too restrictive, the theorem\ndoes not state that matrices with less than *n* distinct eigenvalues\ncannot be diagonalized. The following example illustrates one case in\nwich a matrix does not have *n* distinct eigenvalues and still can be\ndiagonalized.\n\n$$\\mathbf{A} = \n            \\begin{bmatrix}\n            2 & 1 & 1 \\\\\n            0 & 1 & 0 \\\\\n            0 & 0 & 1 \\\\\n            \\end{bmatrix}$$\n\nBy inspection we can verify that $\\lambda_1 = 2$, $\\lambda_2 = 1$, and\n$\\lambda_3 = 1$ are eigenvalues of $\\mathbf{A}$ associated with the\nrespective eigenvectors\n\n$$\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\n\\mathbf{v}_2 = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{v}_3 = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix}$$\n\nbecause each pair $(\\lambda_i, \\mathbf{v}_i)$ satisfies\n$\\mathbf{A} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$. There are $n$\nlinearly independent eigenvectors, i.e., the set\n$\\{ \\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3 \\}$ spans $\\mathbb{R}^3$,\ntherefore a matrix $\\mathbf{V}$ whose columns are the eigenvectors of\n$\\mathbf{A}$ is square and invertible. The similarity transformation\n$\\mathbf{V}^{-1} \\mathbf{A} \\mathbf{V}$ yields a diagonal matrix,\nsimilar to $\\mathbf{A}$.\n\nThe example above is rather special, for ther eare no guarantees that\nmatrices with $r < n$ distinct eigenvalues can be diagonalizes. The\nreader is invited to try to find the eigenvalues and eigenvectors for\nthe matrix **A** in the following example. $$\\mathbf{A} = \n            \\begin{bmatrix}\n            2 & 1 & 1 \\\\\n            0 & 1 & 1 \\\\\n            0 & 0 & 1 \\\\\n            \\end{bmatrix}$$ There are only two invariant directions for\n**A**, $$\\mathbf{v_1} = \n            \\begin{bmatrix}\n            1 \\\\\n            0 \\\\\n            0 \\\\\n            \\end{bmatrix}$$ $$\\mathbf{v_1} = \n            \\begin{bmatrix}\n            1 \\\\\n            -1 \\\\\n            0 \\\\\n            \\end{bmatrix}$$ associated with\n$\\lambda_1 = 2$ and $\\lambda_2 = 1$, respectively.\n\nWe may say that this example is rather special, for even though its\neigenvalues are not distinct and real, the are *n* = 3 linearly\nindependent eigenvectors and the matrix is diagonalizable. We have seen\nthat this is not always the case. However, for one particular set of\nmatrices, the set of symmetric matrices in $\\mathbb{R}^n$, we can always\nfind *n* linearly independent eigenvectors and consequently these\nmatrices are always diagonalizable. In fact, we will see further in the\nbook that this property is enjoyed by the more general class of normal\nmatrices, to which symmetric are a special case. We will defer dealing\nwith complex eigenvalues and eigenvectors to later notebooks",
      "metadata": {}
    },
    {
      "id": "4c5e4fd3-b98d-43a8-9417-f9492e299b7e",
      "cell_type": "markdown",
      "source": "### Diagonalization using Eigenvalues and Eigenvectors\nCalculating the eigenvalues of a matrix **A** is an essential step in the process of diagonalizing this linear operator. From the theory of diagonalization, we know that the diagonal elements of the diagonalized matrix **A** will be its eigenvalues. This is because when a matrix is diagonalized, it is expressed in the form **A = P D P^{-1}**, where **P** is the matrix of eigenvectors and **D** is a diagonal matrix whose entries are the eigenvalues of **A**. \n\nConversely, if we have a diagonalizable matrix, we can also determine its eigenvalues and eigenvectors by performing the diagonalization process. By diagonalizing the matrix, we can directly extract the eigenvalues from the diagonal matrix **D**, and the corresponding eigenvectors will be the columns of the matrix **P**. This two-way relationship between diagonalization, eigenvalues, and eigenvectors makes diagonalization a powerful tool in linear algebra.\n\nIn the example bellow, we'll perform the diagonalization of a 3x3 matrix **A** by, previously, calculating it's eigenvalues and using the ralation presented above.  ",
      "metadata": {}
    },
    {
      "id": "46cf827f-4da3-42a0-973d-e34782c946fe",
      "cell_type": "code",
      "source": "import numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display, Markdown\nfrom ipywidgets import Output\n\n# Define the widgets for input and output\noutput = Output()\n\n# Function to perform diagonalization and display eigenvalues and eigenvectors\ndef diagonalize_matrix(b):\n    np.set_printoptions(precision=4, suppress=True)\n    output.clear_output()\n    try:\n        # Extract the numerical values from the widgets to form the matrix\n        matrix = np.array([[a11.value, a12.value, a13.value], [a21.value, a22.value, a23.value],[a31.value, a32.value, a33.value]])\n        \n        # Check if it's a square matrix\n        if matrix.shape[0] != matrix.shape[1]:\n            raise ValueError(\"The matrix must be square!\")\n        \n        # Calculate eigenvalues and eigenvectors\n        eigenvalues, eigenvectors = np.linalg.eig(matrix)\n        # Calculate the diagonal matrix using eigenvalues\n        diagonal_matrix = np.diag(eigenvalues)\n        \n        with output:\n            display(Markdown(\"#### Matrix Diagonalization\"))\n            display(Markdown(f\"Original Matrix A:\\n{matrix}\"))\n            display(Markdown(f\"Eigenvalues:\\n{eigenvalues}\"))\n            display(Markdown(f\"Eigenvectors (Columns):\\n{eigenvectors}\"))\n            display(Markdown(f\"Diagonal Matrix (using eigenvalues):\\n{diagonal_matrix}\"))\n            display(Markdown(\n                \"Diagonalization simplifies the matrix into a diagonal form, where the eigenvalues are placed on the diagonal. \"\n                \"The eigenvectors represent the directions in which the matrix only scales, making it easier to compute powers of the matrix.\"\n            ))\n    except Exception as e:\n        with output:\n            display(Markdown(f\"**Error:** {e}\"))\n\n# Define the input widgets for matrix elements\na11 = widgets.IntText(value=1, description='', step=1, layout=widgets.Layout(width='50px'))\na12 = widgets.IntText(value=2, description='', step=1, layout=widgets.Layout(width='50px'))\na13 = widgets.IntText(value=3, description='', step=1, layout=widgets.Layout(width='50px'))\na21 = widgets.IntText(value=4, description='', step=1, layout=widgets.Layout(width='50px'))\na22 = widgets.IntText(value=5, description='', step=1, layout=widgets.Layout(width='50px'))\na23 = widgets.IntText(value=6, description='', step=1, layout=widgets.Layout(width='50px'))\na31 = widgets.IntText(value=7, description='', step=1, layout=widgets.Layout(width='50px'))\na32 = widgets.IntText(value=8, description='', step=1, layout=widgets.Layout(width='50px'))\na33 = widgets.IntText(value=9, description='', step=1, layout=widgets.Layout(width='50px'))\n\n# Define the layout for the matrix input widgets\nmatrix_inputs = widgets.GridBox([a11, a12, a13, a21, a22, a23, a31, a32, a33], layout=widgets.Layout(grid_template_columns=\"60px 60px 60px\"))\n# Create the button and bind it to the diagonalization function\nbutton = widgets.Button(description=\"Diagonalize\")\nbutton.on_click(diagonalize_matrix)\n\n# Display the matrix inputs, button, and output area\ndisplay(Markdown(\"#### Matrix A inputs:\"))\ndisplay(matrix_inputs, button, output)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "#### Matrix A inputs:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "GridBox(children=(IntText(value=1, layout=Layout(width='50px')), IntText(value=2, layout=Layout(width='50px'))‚Ä¶",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "014ce4d3fe734ed180cb931d213fe589"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Button(description='Diagonalize', style=ButtonStyle())",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf2d2e92dddc412f93e6eebbbd6584a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Output()",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df8621b12faf45e182aa2e0757fa6443"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 75
    },
    {
      "id": "9be8c73e-9ddb-41fa-816f-6d040a4673a7",
      "cell_type": "markdown",
      "source": "### Questions:\n\n1) What's algebraic and geometric multiplicity?  \n2) If **U** is a vector space with *n* dimension and **A** is a matrix in **U**, what can you infer about the eigenvalues number of **A**?  \n3) Will the linear combination of the eigenvectors of **A**, necessarily, span the vector space **U**?\n",
      "metadata": {}
    },
    {
      "id": "96c60a35-3bb1-4b7e-9fba-d543285fee25",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}